{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, what is the input $\\textbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know about **distributional similarity** (embeddings: --traditional:LSA--, --neural:word2vec--)\n",
    "* understand the difference between **discrete** (one-hot) and **dense** feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Features so far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we go further, lets make a detour and recap: How did we represent a training instance in a traditional classifier so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, recall our example from week 1: training a Logistic Regression classifier for sentiment classification. \n",
    "\n",
    "* Describe in words: what were the features we used? I.e., how did we represent a training instance $\\textbf{x}$?\n",
    "* How can you now describe the entire sentiment training data set as a matrix $X$, i.e.,  what are the rows and columns of $X$? $$ X = \\{\\mathbf{x_1}, ... , \\mathbf{x_n}\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far we used **sparse** inputs (n-hot encodings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**discrete representation**\n",
    "\n",
    "$$\\mathbf{x}_{cat} = [0,0,0,0,0,0,1] $$\n",
    "$$\\mathbf{x}_{dog} = [0,0,0,0,1,0,0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**similarity** on discrete representations? $$\\mathbf{x}_{cat} \\wedge \\mathbf{x}_{dog} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Probably the biggest jump when moving from traditional linear models with sparse inputs to deep neural networks is to stop representing each feature as a unique dimension, but instead represent them as **dense vectors** (Goldberg, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of using discrete representations, we will **embed** words into a high-dimensional feature space and represent each word by a lower-dimensional dense *vector* (aka. *embedding*):\n",
    "<img src=\"http://ben.bolte.cc/resources/attention_rnn/word_vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>**\"You shall know a word by the company it keeps\"** (Firth, J. R. 1957:11)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/flÃ¸debolle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"The company it keeps\": word co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent the \"company\" of a word in terms of a word co-occurence matrix. On the rows we have the words, on the columns their context.\n",
    "\n",
    "**Contexts** can be of different types, for example:\n",
    "* entire documents\n",
    "* paragraphs\n",
    "* a window around the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"I like Groningen .\", \"I like good food .\", \"I enjoy flying .\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Groningen', 'like', 'flying', 'I', '.', 'enjoy', 'food', 'good'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = set(np.concatenate([s.split() for s in corpus],0))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  1.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  1.  0.]]\n",
      "{'.': 0, 'Groningen': 1, 'I': 2, 'enjoy': 3, 'flying': 4, 'food': 5, 'good': 6, 'like': 7}\n"
     ]
    }
   ],
   "source": [
    "# lets build a co-occurence matrix \n",
    "# rows: indices of words\n",
    "# columns: each column is a document, register whether the word appeared in the context\n",
    "## (in practice: many more context, different weighting schemes etc..)\n",
    "w2i = {w: i for i,w in enumerate(sorted(vocab))}\n",
    "coocurrence_matrix = np.zeros((len(vocab),len(corpus)))\n",
    "for col_idx, sentence in enumerate(corpus):\n",
    "    sentence = sentence.split()\n",
    "    for word in sentence:\n",
    "        word_idx = w2i[word]\n",
    "        coocurrence_matrix[(word_idx,col_idx)] +=1\n",
    "print(coocurrence_matrix)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Co-occurence matrix\n",
    "\n",
    "* **dimensionality**: number of words $|V|$ (size of vocabulary) times number of documents (typically number of documents is huge)\n",
    "* we want to **reduce** its dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSA - Latent Semantic Analysis (Singular Value Decomposition - SVD)\n",
    "\n",
    "Approximate a matrix $\\mathbf{C}$ through a decomposition into three submatrices (**of smaller dimensionality**):\n",
    "\n",
    "$$\\mathbf{C} \\approx \\mathbf{U \\sum V^T}$$\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\">\n",
    "\n",
    "NB. $=$ should be $\\approx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# reduce space to, say, 2 dimensions (for simplicity here)\n",
    "U, Sigma, VT = randomized_svd(coocurrence_matrix, \n",
    "                              n_components=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualizing the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1100d2630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of U: (8, 2)\n",
      "vector for 'like': [ 0.40929765 -0.41256711]\n",
      "vector for 'enjoy': [ 0.16047237  0.54375519]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words = list(vocab)\n",
    "for i, lab in enumerate(vocab):\n",
    "    plt.text(U[i,0],U[i,1], words[i])\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()\n",
    "print(\"size of U:\", U.shape)\n",
    "print(\"vector for 'like':\", U[w2i[\"like\"]])\n",
    "print(\"vector for 'enjoy':\", U[w2i[\"enjoy\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity\n",
    "\n",
    "**cosine** similarity \n",
    "\n",
    "(it ranges from -1 to 1; is 1 if vectors are the same, 0 if they are independent, and -1 if they are exactly opposite)\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/eq1.png\">\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/vector_example2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Calculate the cosine distances between the words *enjoy* and *like*, *good* and *enjoy* as well as *good* and *Groningen*. (Hint: use *cosine* function from *scipy.spatial.distance*). What is the distance between a word and itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like<>enjoy:     1.48\n",
      "good<>enjoy:     1.62\n",
      "good<>Groningen: 0.11\n",
      "0.00\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "v_like = U[w2i[\"like\"]]\n",
    "v_enjoy = U[w2i[\"enjoy\"]]\n",
    "v_good = U[w2i[\"good\"]]\n",
    "\n",
    "\n",
    "print(\"like<>enjoy:     {0:.2f}\".format(cosine(v_like, v_enjoy)))\n",
    "print(\"good<>enjoy:     {0:.2f}\".format(cosine(v_good, v_enjoy)))\n",
    "print(\"good<>Groningen: {0:.2f}\".format(cosine(v_good, U[w2i[\"Groningen\"]])))\n",
    "\n",
    "print(\"{0:.2f}\".format(cosine(v_like, v_like)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning approach: Directly learning word vectors (embeddings)\n",
    "\n",
    "* SVD: computation cost scales quadratically with size of co-occurence matrix; difficult to integrate new words\n",
    "* **Idea**: directly learn word vectors (word2vec)\n",
    "    * NLP (almost) from Scratch (Collobert & Weston, 2008)\n",
    "    * word2vec (Mikolov et al, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main idea of word2vec\n",
    "\n",
    "* instead of capturing co-occurence statistics of words\n",
    "* **predict context** (surrounding words of every word); in particular, predict words in a window of length $m$ around current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$o$ is the outside word (context), $c$ is the current center word; \n",
    "\n",
    "Maximize the probability of a word in the context ($o$) given the current word $c$:\n",
    "\n",
    "$$p(o|c) = \\frac{exp(u_o^T v_c)}{\\sum_{w=1}^W exp(u_w^T v_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://www.gabormelli.com/RKB/images/a/a6/skip-gram_NNLM_architecture.150216.jpg\" width=500>\n",
    "\n",
    "At the end you can read off the embedding vector from the Embedding layer! voila!\n",
    "\n",
    "NB. denominator $\\sum$ over all words! In practice, *negative sampling* is used (randomly choose a word which is not in context as a negative sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In deep learning we represent words as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**a) sparse representation vs b) dense representation**  (Figure 1 in Yoav Goldberg's primer)\n",
    "<img src=\"pics/sparsevsdense.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: animacy classification\n",
    "\n",
    "Exercise: \n",
    "\n",
    "* add an embedding layer to the animacy classification example. For now use a simple concatenation as representation (Flatten the embedding layer)\n",
    "\n",
    "* add code that reads off the embedding layer from the network and stores in in a file \"vectors.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Florida', 0.9196957945823669),\n",
       " ('Arlington', 0.9070820808410645),\n",
       " ('California', 0.8937997817993164),\n",
       " ('Buffalo', 0.8546850085258484),\n",
       " ('Canada', 0.8518165349960327),\n",
       " ('Japan', 0.8483419418334961),\n",
       " ('Europe', 0.8347345590591431),\n",
       " ('Houston', 0.8322141170501709),\n",
       " ('Virginia', 0.8314700126647949),\n",
       " ('France', 0.8283012509346008)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once we have read off the embeddings after training the animacy classifier, and \n",
    "# stored them in file 'vectors.txt' we load it for inspection\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['Texas'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elect', 0.851630449295044),\n",
       " ('word', 0.8355662822723389),\n",
       " ('letting', 0.8245967030525208),\n",
       " ('teach', 0.8199971914291382),\n",
       " ('swaying', 0.81275475025177),\n",
       " ('parks', 0.811647891998291),\n",
       " ('finally', 0.7990972995758057),\n",
       " ('wishes', 0.793840765953064),\n",
       " ('repainted', 0.7927937507629395),\n",
       " ('jus-', 0.787428617477417)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['send'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These vectors are not traditional word vectors learned with word2vec (skipgrams), instead we read them off from our animacy classifier (they are not trained with the word2vec objective, but are a by-product from the classifier). Nevertheless this shows us that we can also get embeddings from a neural network with dense (embedding) inputs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Traditional vs deep learning approach to feature extraction (representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The common pipeline of extracting features **for an NLP model with a Neural Network** then becomes:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a **vector** for **each feature** (lookup Embedding table)\n",
    "* **combine** vectors of features to get the vector representation for the **instance** $\\mathbf{x}$ (**dense representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets compare this to our traditional approach - the common pipeline of extracting features for an NLP model is:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector whose length is the total number of features with a 1 at position k if the k-th feature is active; this feature vector represents the **instance** $\\mathbf{x}$  (**sparse representation**, n-hot encoding)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "Now it should be clear why it is called sparse vs dense feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you combine different feature vector representations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In an NLP application, $\\mathbf{x}$ is usually composed of various embedding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the notation in Goldberg (2015), chapter 4, lets use the function $c(\\cdot)$ as **feature combiner** that creates our input embeddings layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common choice for $c$ is **concatenation**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f_3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "This is what happens if we use **Flatten** in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, $c$ could be the **sum of the embeddings vector**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1)+v(f_2)+v(f_3)] $\n",
    "\n",
    "or the **mean**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [mean(v(f_1),v(f_2),v(f_3))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In many papers $v$ is often referred to as the embeddings layer or lookup layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our example from before with explicit input representation\n",
    "\n",
    "For instance, let us explicitly state the input representation. Suppose we use the concatentation operator, then our network above becomes:\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "since: \n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "then: \n",
    "\n",
    "$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{[v(f_1); v(f_2); v(f_3)]W^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As computational graph:\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values of the *embedding vectors* (values of the vectors in Fig 1 b)) are treated as model parameters and trained together with the other parameters of the model (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unrolled (graph with concrete input, expected output, and loss node, Goldberg Figure 3 c):\n",
    "<img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, in deep learning approaches to NLP words are represented as dense vectors. Where do these word vectors (embeddings) come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **randomly initialized** (small numbers around 0) and *trained with the network*\n",
    "* **off-the-shelf embeddings**: you can also use already trained, available embeddings (e.g. estimated with *word2vec*) and *initialize* the embedding layer of the network with your pretrained (unsupervised) word embeddings\n",
    "* **task-specific embeddings**: you could also train your embeddings, read them off the network, and use them for another task (or in a multi-task setup, more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In our animacy classification example we have one simplification: the input is always of the same size (namely, 5 words). \n",
    "\n",
    "However, in NLP we typically never have fixed size inputs, sentences are of different length. The neural network however needs inputs of fixed size. So how to deal with it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* create a mean embedding vector\n",
    "* pad all instances to the same size\n",
    "* use a model that can deal with variable size inputs, like a recurrent neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Karpathy's illustration of RNNs:\n",
    "<img src=\"http://benjaminbolte.com/resources/attention_rnn/karpathy_rnn.jpeg\">\n",
    "\n",
    "* From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important concepts\n",
    "\n",
    "In NLP we typically deal with the following **prediction problems** - Given $x$, predict $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "| Given $x$ | predict $y$  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   a book review  | positive, negative | **classification** (binary) |\n",
    "|   a tweet  | language | **multi-class classification** (several choices) |\n",
    "|   a sentence  | its syntactic parse tree | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sequence tagging is also a structured prediction problem.\n",
    "\n",
    "For a sequence of n words with just 2 possible tags, how many possible sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "| Example task | Traditional classifier  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   sentiment | Logistic regression, SVM | **classification** (binary) |\n",
    "|   language identification  | Logistic regression, SVM  | **multi-class classification** (several choices) |\n",
    "|   POS sequence  | HMM, structured perceptron, (window-based classifier) | **structured prediction** (millions of choices) |\n",
    "|   NER  | CRF, structured perceptron | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 2 and 5: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Simon Paarlberg's [blog on LSA](https://simonpaarlberg.com/post/latent-semantic-analyses/)\n",
    "* Richard Socher's [lecture 2](https://www.youtube.com/watch?v=xhHOL3TNyJs)\n",
    "* Graham Neubig's slides on the [structured perceptron](http://www.phontron.com/slides/nlp-programming-en-12-struct.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
