{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important concepts of lecture 4:\n",
    "\n",
    "* from n-hot vectors (sparse) to embeddings (dense inputs)\n",
    "* baselines for your projects (classification, sequence prediction)\n",
    "* short introduction to RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, what is the input $\\textbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know about **distributional similarity** (embeddings: --traditional:LSA--, --neural:word2vec--)\n",
    "* understand the difference between **discrete** (one-hot) and **dense** feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Features so far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we go further, lets make a detour and recap: How did we represent a training instance in a traditional classifier so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, recall our example from week 1: training a Logistic Regression classifier for sentiment classification. \n",
    "\n",
    "* Describe in words: what were the features we used? I.e., how did we represent a training instance $\\textbf{x}$?\n",
    "* How can you now describe the entire sentiment training data set as a matrix $X$, i.e.,  what are the rows and columns of $X$? $$ X = \\{\\mathbf{x_1}, ... , \\mathbf{x_n}\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far we used **sparse** inputs (n-hot encodings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**discrete representation**\n",
    "\n",
    "$$\\mathbf{x}_{cat} = [0,0,0,0,0,0,1] $$\n",
    "$$\\mathbf{x}_{dog} = [0,0,0,0,1,0,0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**similarity** on discrete representations? $$\\mathbf{x}_{cat} \\wedge \\mathbf{x}_{dog} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Probably the biggest jump when moving from traditional linear models with sparse inputs to deep neural networks is to stop representing each feature as a unique dimension, but instead represent them as **dense vectors** (Goldberg, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of using discrete representations, we will **embed** words into a high-dimensional feature space and represent each word by a lower-dimensional dense *vector* (aka. *embedding*):\n",
    "<img src=\"http://ben.bolte.cc/resources/attention_rnn/word_vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>**\"You shall know a word by the company it keeps\"** (Firth, J. R. 1957:11)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/flÃ¸debolle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"The company it keeps\": word co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent the \"company\" of a word in terms of a word co-occurence matrix. On the rows we have the words, on the columns their context.\n",
    "\n",
    "**Contexts** can be of different types, for example:\n",
    "* entire documents\n",
    "* paragraphs\n",
    "* a window around the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"I like Groningen .\", \"I like good food .\", \"I enjoy flying .\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enjoy', '.', 'Groningen', 'food', 'good', 'flying', 'I', 'like'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = set(np.concatenate([s.split() for s in corpus],0))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  1.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  1.  0.]]\n",
      "{'.': 0, 'Groningen': 1, 'I': 2, 'enjoy': 3, 'flying': 4, 'food': 5, 'good': 6, 'like': 7}\n"
     ]
    }
   ],
   "source": [
    "# lets build a co-occurence matrix \n",
    "# rows: indices of words\n",
    "# columns: each column is a document, register whether the word appeared in the context\n",
    "## (in practice: many more context, different weighting schemes etc..)\n",
    "w2i = {w: i for i,w in enumerate(sorted(vocab))}\n",
    "coocurrence_matrix = np.zeros((len(vocab),len(corpus)))\n",
    "for col_idx, sentence in enumerate(corpus):\n",
    "    sentence = sentence.split()\n",
    "    for word in sentence:\n",
    "        word_idx = w2i[word]\n",
    "        coocurrence_matrix[(word_idx,col_idx)] +=1\n",
    "print(coocurrence_matrix)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Co-occurence matrix\n",
    "\n",
    "* **dimensionality**: number of words $|V|$ (size of vocabulary) times number of documents (typically number of documents is huge)\n",
    "* we want to **reduce** its dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSA - Latent Semantic Analysis (Singular Value Decomposition - SVD)\n",
    "\n",
    "Approximate a matrix $\\mathbf{C}$ through a decomposition into three submatrices (**of smaller dimensionality**):\n",
    "\n",
    "$$\\mathbf{C} \\approx \\mathbf{U \\sum V^T}$$\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\">\n",
    "\n",
    "NB. $=$ should be $\\approx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# reduce space to, say, 2 dimensions (for simplicity here)\n",
    "U, Sigma, VT = randomized_svd(coocurrence_matrix, \n",
    "                              n_components=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualizing the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMhqwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11579e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of U: (8, 2)\n",
      "vector for 'like': [ 0.40929765 -0.41256711]\n",
      "vector for 'enjoy': [ 0.16047237  0.54375519]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words = list(vocab)\n",
    "for i, lab in enumerate(vocab):\n",
    "    plt.text(U[i,0],U[i,1], words[i])\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()\n",
    "print(\"size of U:\", U.shape)\n",
    "print(\"vector for 'like':\", U[w2i[\"like\"]])\n",
    "print(\"vector for 'enjoy':\", U[w2i[\"enjoy\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity\n",
    "\n",
    "**cosine** similarity \n",
    "\n",
    "(it ranges from -1 to 1; is 1 if vectors are the same, 0 if they are independent, and -1 if they are exactly opposite)\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/eq1.png\">\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/vector_example2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Calculate the cosine distances between the words *enjoy* and *like*, *good* and *enjoy* as well as *good* and *Groningen*. (Hint: use *cosine* function from *scipy.spatial.distance*). What is the distance between a word and itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like<>enjoy:     1.48\n",
      "good<>enjoy:     1.62\n",
      "good<>Groningen: 0.11\n",
      "0.00\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "v_like = U[w2i[\"like\"]]\n",
    "v_enjoy = U[w2i[\"enjoy\"]]\n",
    "v_good = U[w2i[\"good\"]]\n",
    "\n",
    "\n",
    "print(\"like<>enjoy:     {0:.2f}\".format(cosine(v_like, v_enjoy)))\n",
    "print(\"good<>enjoy:     {0:.2f}\".format(cosine(v_good, v_enjoy)))\n",
    "print(\"good<>Groningen: {0:.2f}\".format(cosine(v_good, U[w2i[\"Groningen\"]])))\n",
    "\n",
    "print(\"{0:.2f}\".format(cosine(v_like, v_like)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning approach: Directly learning word vectors (embeddings)\n",
    "\n",
    "* SVD: computation cost scales quadratically with size of co-occurence matrix; difficult to integrate new words\n",
    "* **Idea**: directly learn word vectors (word2vec)\n",
    "    * NLP (almost) from Scratch (Collobert & Weston, 2008)\n",
    "    * word2vec (Mikolov et al, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main idea of word2vec\n",
    "\n",
    "* instead of capturing co-occurence statistics of words\n",
    "* **predict context** (surrounding words of every word); in particular, predict words in a window of length $m$ around current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$o$ is the outside word (context), $c$ is the current center word; \n",
    "\n",
    "Maximize the probability of a word in the context ($o$) given the current word $c$:\n",
    "\n",
    "$$p(o|c) = \\frac{exp(u_o^T v_c)}{\\sum_{w=1}^W exp(u_w^T v_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://www.gabormelli.com/RKB/images/a/a6/skip-gram_NNLM_architecture.150216.jpg\" width=500>\n",
    "\n",
    "At the end you can read off the embedding vector from the Embedding layer! voila!\n",
    "\n",
    "NB. denominator $\\sum$ over all words! In practice, *negative sampling* is used (randomly choose a word which is not in context as a negative sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In deep learning we represent words as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**a) sparse representation vs b) dense representation**  (Figure 1 in Yoav Goldberg's primer)\n",
    "<img src=\"pics/sparsevsdense.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Traditional vs deep learning approach to feature extraction (representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The common pipeline of extracting features **for an NLP model with a Neural Network** then becomes:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a **vector** for **each feature** (lookup Embedding table)\n",
    "* **combine** vectors of features to get the vector representation for the **instance** $\\mathbf{x}$ (**dense representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets compare this to our traditional approach - the common pipeline of extracting features for an NLP model is:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector whose length is the total number of features with a 1 at position k if the k-th feature is active; this feature vector represents the **instance** $\\mathbf{x}$  (**sparse representation**, n-hot encoding)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "Now it should be clear why it is called sparse vs dense feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you combine different feature vector representations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In an NLP application, $\\mathbf{x}$ is usually composed of various embedding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the notation in Goldberg (2015), chapter 4, lets use the function $c(\\cdot)$ as **feature combiner** that creates our input embeddings layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common choice for $c$ is **concatenation**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f_3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "This is what happens if we use **Flatten** in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, $c$ could be the **sum of the embeddings vector**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1)+v(f_2)+v(f_3)] $\n",
    "\n",
    "or the **mean**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [mean(v(f_1),v(f_2),v(f_3))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In many papers $v$ is often referred to as the embeddings layer or lookup layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our example from before with explicit input representation\n",
    "\n",
    "For instance, let us explicitly state the input representation. Suppose we use the concatentation operator, then our network above becomes:\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "since: \n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "then: \n",
    "\n",
    "$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{[v(f_1); v(f_2); v(f_3)]W^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As computational graph:\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values of the *embedding vectors* (values of the vectors in Fig 1 b)) are treated as model parameters and trained together with the other parameters of the model (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unrolled (graph with concrete input, expected output, and loss node, Goldberg Figure 3 c):\n",
    "<img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: animacy classification\n",
    "\n",
    "Exercise: \n",
    "\n",
    "* add an embedding layer to the animacy classification example. For now use a simple concatenation as representation (Flatten the embedding layer). What performance do you get?\n",
    "\n",
    "* add code that reads off the embedding layer from the network and stores in in a file \"vectors.txt\". Once you have this embedding vector you can inspect it (find nearest neighbors) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Florida', 0.9196957945823669),\n",
       " ('Arlington', 0.9070820808410645),\n",
       " ('California', 0.8937997817993164),\n",
       " ('Buffalo', 0.8546850085258484),\n",
       " ('Canada', 0.8518165349960327),\n",
       " ('Japan', 0.8483419418334961),\n",
       " ('Europe', 0.8347345590591431),\n",
       " ('Houston', 0.8322141170501709),\n",
       " ('Virginia', 0.8314700126647949),\n",
       " ('France', 0.8283012509346008)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once we have read off the embeddings after training the animacy classifier, and \n",
    "# stored them in file 'vectors.txt' we load it for inspection\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['Texas'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elect', 0.851630449295044),\n",
       " ('word', 0.8355662822723389),\n",
       " ('letting', 0.8245967030525208),\n",
       " ('teach', 0.8199971914291382),\n",
       " ('swaying', 0.81275475025177),\n",
       " ('parks', 0.811647891998291),\n",
       " ('finally', 0.7990972995758057),\n",
       " ('wishes', 0.793840765953064),\n",
       " ('repainted', 0.7927937507629395),\n",
       " ('jus-', 0.787428617477417)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['send'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These vectors are not traditional word vectors learned with word2vec (skipgrams), instead we read them off from our animacy classifier (they are not trained with the word2vec objective, but are a by-product from the classifier). Nevertheless this shows us that we can also get embeddings from a neural network with dense (embedding) inputs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, in deep learning approaches to NLP words are represented as dense vectors. Where do these word vectors (embeddings) come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **randomly initialized** (small numbers around 0) and *trained with the network*\n",
    "* **off-the-shelf embeddings**: you can also use already trained, available embeddings (e.g. estimated with *word2vec*) and *initialize* the embedding layer of the network with your pretrained (unsupervised) word embeddings\n",
    "* **task-specific embeddings**: you could also train your embeddings, read them off the network, and use them for another task (or in a multi-task setup, more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our animacy classification example we have one simplification: the input is always of the same size (namely, 5 words). \n",
    "\n",
    "However, in NLP we typically never have fixed size inputs, sentences are of different length. The neural network however needs inputs of fixed size. So how to deal with it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* create an input of fixed size, like using the mean embedding vector\n",
    "* use a model that can deal with variable size inputs, like a recurrent neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Karpathy's illustration of RNNs:\n",
    "<img src=\"http://benjaminbolte.com/resources/attention_rnn/karpathy_rnn.jpeg\">\n",
    "\n",
    "* From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important concepts: Prediction problems, non-neural baselines\n",
    "\n",
    "In NLP we typically deal with the following **prediction problems** - Given $x$, predict $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "| Given $x$ | predict $y$  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   a book review  | positive, negative | **classification** (binary) |\n",
    "|   a tweet  | language | **multi-class classification** (several choices) |\n",
    "|   a sentence  | its syntactic parse tree | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sequence tagging is also a structured prediction problem.\n",
    "\n",
    "For a sequence of n words with just 2 possible tags, how many possible sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "| Example task | Traditional classifier  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   sentiment | Logistic regression, SVM | **classification** (binary) |\n",
    "|   language identification  | Logistic regression, SVM  | **multi-class classification** (several choices) |\n",
    "|   POS sequence  | HMM, structured perceptron, (window-based classifier) | **structured prediction** (millions of choices) |\n",
    "|   NER  | CRF, structured perceptron | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 2 and 5: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Simon Paarlberg's [blog on LSA](https://simonpaarlberg.com/post/latent-semantic-analyses/)\n",
    "* Richard Socher's [lecture 2](https://www.youtube.com/watch?v=xhHOL3TNyJs)\n",
    "* Graham Neubig's slides on the [structured perceptron](http://www.phontron.com/slides/nlp-programming-en-12-struct.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
