{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important concepts of lecture 4:\n",
    "\n",
    "* from n-hot vectors (sparse) to embeddings (dense inputs)\n",
    "* baselines for your projects (classification, sequence prediction)\n",
    "* short introduction to RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, what is the input $\\textbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Features so far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we go further, lets make a detour and recap: How did we represent a training instance in a traditional classifier so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, recall our example from week 1: training a Logistic Regression classifier for sentiment classification. \n",
    "\n",
    "* Describe in words: what were the features we used? I.e., how did we represent a training instance $\\textbf{x}$?\n",
    "* How can you now describe the entire sentiment training data set as a matrix $X$, i.e.,  what are the rows and columns of $X$? $$ X = \\{\\mathbf{x_1}, ... , \\mathbf{x_n}\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far we used **sparse** inputs (n-hot encodings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know about **distributional similarity** (embeddings: --traditional:LSA--, --neural:word2vec--)\n",
    "* understand the difference between **discrete** (one-hot) and **dense** feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**discrete representation**\n",
    "\n",
    "$$\\mathbf{x}_{cat} = [0,0,0,0,0,0,1] $$\n",
    "$$\\mathbf{x}_{dog} = [0,0,0,0,1,0,0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**similarity** on discrete representations? $$\\mathbf{x}_{cat} \\wedge \\mathbf{x}_{dog} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Probably the biggest jump when moving from traditional linear models with sparse inputs to deep neural networks is to stop representing each feature as a unique dimension, but instead represent them as **dense vectors** (Goldberg, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of using discrete representations, we will **embed** words into a high-dimensional feature space and represent each word by a lower-dimensional dense *vector* (aka. *embedding*):\n",
    "<img src=\"http://ben.bolte.cc/resources/attention_rnn/word_vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>**\"You shall know a word by the company it keeps\"** (Firth, J. R. 1957:11)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/flÃ¸debolle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"The company it keeps\": word co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent the \"company\" of a word in terms of a word co-occurence matrix. On the rows we have the words, on the columns their context.\n",
    "\n",
    "**Contexts** can be of different types, for example:\n",
    "* entire documents\n",
    "* paragraphs\n",
    "* a window around the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"She enjoys Groningen .\", \"She likes Cockatoos .\", \"She likes good food\", \"I like Groningen .\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'likes', 'I', 'enjoys', 'Cockatoos', '.', 'good', 'She', 'like', 'food', 'Groningen'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = set(np.concatenate([s.split() for s in corpus],0))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  10\n",
      "[[ 1.  1.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  1.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# lets build a co-occurence matrix \n",
    "# rows: indices of words\n",
    "# columns: each column is a document, register whether the word appeared in the context\n",
    "## (in practice: many more context, different weighting schemes etc..)\n",
    "w2i = {w: i for i,w in enumerate(sorted(vocab))}\n",
    "i2w = {i: w for i,w in enumerate(w2i)}\n",
    "\n",
    "coocurrence_matrix = np.zeros((len(vocab),len(corpus)))\n",
    "for col_idx, sentence in enumerate(corpus):\n",
    "    sentence = sentence.split()\n",
    "    for word in sentence:\n",
    "        word_idx = w2i[word]\n",
    "        coocurrence_matrix[(word_idx,col_idx)] +=1\n",
    "\n",
    "print(\"vocab size: \", len(w2i))\n",
    "print(coocurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with row info:\n",
      "0\t.\t[ 1.  1.  0.  1.]\n",
      "1\tCockatoos\t[ 0.  1.  0.  0.]\n",
      "2\tGroningen\t[ 1.  0.  0.  1.]\n",
      "3\tI\t[ 0.  0.  0.  1.]\n",
      "4\tShe\t[ 1.  1.  1.  0.]\n",
      "5\tenjoys\t[ 1.  0.  0.  0.]\n",
      "6\tfood\t[ 0.  0.  1.  0.]\n",
      "7\tgood\t[ 0.  0.  1.  0.]\n",
      "8\tlike\t[ 0.  0.  0.  1.]\n",
      "9\tlikes\t[ 0.  1.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"with row info:\")\n",
    "for i, row in enumerate(coocurrence_matrix):\n",
    "    print(\"{}\\t{}\\t{}\".format(i, i2w[i], row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Co-occurence matrix\n",
    "\n",
    "* **dimensionality**: number of words $|V|$ (size of vocabulary) times number of documents (typically number of documents is huge)\n",
    "* we want to **reduce** its dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSA - Latent Semantic Analysis (Singular Value Decomposition - SVD)\n",
    "\n",
    "Approximate a matrix $\\mathbf{C}$ through a decomposition into three submatrices (**of smaller dimensionality**):\n",
    "\n",
    "$$\\mathbf{C} \\approx \\mathbf{U \\sum V^T}$$\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\">\n",
    "\n",
    "NB. $=$ should be $\\approx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# reduce space to, say, 2 dimensions (for simplicity here)\n",
    "U, Sigma, VT = randomized_svd(coocurrence_matrix, \n",
    "                              n_components=2)\n",
    "print(U.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualizing the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1181e36a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of U: (10, 2)\n",
      "vector for 'likes': [ 0.34550151  0.43973261]\n",
      "vector for 'enjoys': [ 0.20078434 -0.1287947 ]\n",
      "9\n",
      "[ 0.34550151  0.43973261]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words = list(vocab)\n",
    "for i, lab in enumerate(vocab):\n",
    "    plt.text(U[i,0],U[i,1], words[i])\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()\n",
    "print(\"size of U:\", U.shape)\n",
    "print(\"vector for 'likes':\", U[w2i[\"likes\"]])\n",
    "print(\"vector for 'enjoys':\", U[w2i[\"enjoys\"]])\n",
    "print(w2i['likes'])\n",
    "print(U[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity\n",
    "\n",
    "**cosine** similarity \n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/eq1.png\">\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/vector_example2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Calculate the cosine distance between the words *good* and *enjoys* as well as *enjoys* and *likes*. (Hint: you can use the *cosine* **distance** function from *scipy.spatial.distance*, notice it is 1 minus cosine similarity). What is the distance between a word and itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distances:\n",
      "good<>enjoys:     1.13\n",
      "enjoys<>likes: 0.90\n",
      "good<>good: 0.00\n"
     ]
    }
   ],
   "source": [
    "## solution:\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "v_likes = U[w2i[\"likes\"]]\n",
    "v_enjoys = U[w2i[\"enjoys\"]]\n",
    "v_good = U[w2i[\"good\"]]\n",
    "\n",
    "print(\"cosine distances:\")\n",
    "print(\"good<>enjoys:     {0:.2f}\".format(cosine(v_good, v_enjoys)))\n",
    "print(\"enjoys<>likes: {0:.2f}\".format(cosine(v_enjoys, v_likes)))\n",
    "print(\"good<>good: {0:.2f}\".format(cosine(v_good, v_good)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning approach: Directly learning word vectors (embeddings)\n",
    "\n",
    "* SVD: computation cost scales quadratically with size of co-occurence matrix; difficult to integrate new words\n",
    "* **Idea**: directly learn word vectors (word2vec)\n",
    "    * NLP (almost) from Scratch (Collobert & Weston, 2008)\n",
    "    * word2vec (Mikolov et al, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main idea of word2vec\n",
    "\n",
    "* instead of capturing co-occurence statistics of words\n",
    "* **predict context** (surrounding words of every word); in particular, predict words in a window of length $m$ around current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$o$ is the outside word (context), $c$ is the current center word; \n",
    "\n",
    "Maximize the probability of a word in the context ($o$) given the current word $c$:\n",
    "\n",
    "$$p(o|c) = \\frac{exp(u_o^T v_c)}{\\sum_{w=1}^W exp(u_w^T v_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://www.gabormelli.com/RKB/images/a/a6/skip-gram_NNLM_architecture.150216.jpg\" width=500>\n",
    "\n",
    "At the end you can read off the embedding vector from the Embedding layer! voila!\n",
    "\n",
    "NB. denominator $\\sum$ over all words! In practice, *negative sampling* is used (randomly choose a word which is not in context as a negative sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In deep learning we represent words as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**a) sparse representation vs b) dense representation**  (Figure 1 in Yoav Goldberg's primer)\n",
    "<img src=\"pics/sparsevsdense.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Traditional vs deep learning approach to feature extraction (representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The common pipeline of extracting features **for an NLP model with a Neural Network** then becomes:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a **vector** for **each feature** (lookup Embedding table)\n",
    "* **combine** vectors of features to get the vector representation for the **instance** $\\mathbf{x}$ (**dense representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets compare this to our traditional approach - the common pipeline of extracting features for an NLP model is:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector whose length is the total number of features with a 1 at position k if the k-th feature is active; this feature vector represents the **instance** $\\mathbf{x}$  (**sparse representation**, n-hot encoding)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "Now it should be clear why it is called sparse vs dense feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you combine different feature vector representations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In an NLP application, $\\mathbf{x}$ is usually composed of various embedding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the notation in Goldberg (2015), chapter 4, lets use the function $c(\\cdot)$ as **feature combiner** that creates our input embeddings layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common choice for $c$ is **concatenation**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f_3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "This is what happens if we use **Flatten** in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, $c$ could be the **sum of the embeddings vector**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1)+v(f_2)+v(f_3)] $\n",
    "\n",
    "or the **mean**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [mean(v(f_1),v(f_2),v(f_3))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In many papers $v$ is often referred to as the embeddings layer or lookup layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our example from before with explicit input representation\n",
    "\n",
    "For instance, let us explicitly state the input representation. Suppose we use the concatentation operator, then our network above becomes:\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "since: \n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "then: \n",
    "\n",
    "$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{[v(f_1); v(f_2); v(f_3)]W^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As computational graph:\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values of the *embedding vectors* (values of the vectors in Fig 1 b)) are treated as model parameters and trained together with the other parameters of the model (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unrolled (graph with concrete input, expected output, and loss node, Goldberg Figure 3 c):\n",
    "<img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: animacy classification\n",
    "\n",
    "Exercise: \n",
    "\n",
    "* add an embedding layer to the animacy classification example. For now use a simple concatenation as representation (Flatten the embedding layer). What performance do you get?\n",
    "\n",
    "* add code that reads off the embedding layer from the network and stores in in a file \"vectors.txt\". Once you have this embedding vector you can inspect it (find nearest neighbors) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Florida', 0.9196957945823669),\n",
       " ('Arlington', 0.9070820808410645),\n",
       " ('California', 0.8937997817993164),\n",
       " ('Buffalo', 0.8546850085258484),\n",
       " ('Canada', 0.8518165349960327),\n",
       " ('Japan', 0.8483419418334961),\n",
       " ('Europe', 0.8347345590591431),\n",
       " ('Houston', 0.8322141170501709),\n",
       " ('Virginia', 0.8314700126647949),\n",
       " ('France', 0.8283012509346008)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once we have read off the embeddings after training the animacy classifier, and \n",
    "# stored them in file 'vectors.txt' we load it for inspection\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['Texas'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elect', 0.851630449295044),\n",
       " ('word', 0.8355662822723389),\n",
       " ('letting', 0.8245967030525208),\n",
       " ('teach', 0.8199971914291382),\n",
       " ('swaying', 0.81275475025177),\n",
       " ('parks', 0.811647891998291),\n",
       " ('finally', 0.7990972995758057),\n",
       " ('wishes', 0.793840765953064),\n",
       " ('repainted', 0.7927937507629395),\n",
       " ('jus-', 0.787428617477417)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['send'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These vectors are not traditional word vectors learned with word2vec (skipgrams), instead we read them off from our animacy classifier (they are not trained with the word2vec objective, but are a by-product from the classifier). Nevertheless this shows us that we can also get embeddings from a neural network with dense (embedding) inputs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, in deep learning approaches to NLP words are represented as dense vectors. Where do these word vectors (embeddings) come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **randomly initialized** (small numbers around 0) and *trained with the network*\n",
    "* **off-the-shelf embeddings**: you can also use already trained, available embeddings (e.g. estimated with *word2vec*) and *initialize* the embedding layer of the network with your pretrained (unsupervised) word embeddings\n",
    "* **task-specific embeddings**: you could also train your embeddings, read them off the network, and use them for another task (or in a multi-task setup, more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inputs of different lengths\n",
    "\n",
    "In our animacy classification example we have one simplification: the input is always of the same size (namely, 5 words). \n",
    "\n",
    "However, in NLP we typically never have fixed size inputs, sentences are of different length. The neural network however needs inputs of fixed size. So how to deal with it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* create an input of fixed size, like using the mean embedding vector\n",
    "* use a model that can deal with variable size inputs, like a recurrent neural network (depending on the deep learning toolkit you use, you might still need to *pad* sequences to a fixed length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Karpathy's illustration of RNNs:\n",
    "<img src=\"http://benjaminbolte.com/resources/attention_rnn/karpathy_rnn.jpeg\">\n",
    "\n",
    "* From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important concepts: Prediction problems, non-neural baselines\n",
    "\n",
    "In NLP we typically deal with the following **prediction problems** - Given $x$, predict $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "| Given $x$ | predict $y$  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   a book review  | positive, negative | **classification** (binary) |\n",
    "|   a tweet  | language | **multi-class classification** (several choices) |\n",
    "|   a sentence  | its syntactic parse tree | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sequence tagging is also a structured prediction problem.\n",
    "\n",
    "For a sequence of n words with just 2 possible tags, how many possible sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "| Example task | Traditional classifier  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   sentiment | Logistic regression, SVM | **classification** (binary) |\n",
    "|   language identification  | Logistic regression, SVM  | **multi-class classification** (several choices) |\n",
    "|   POS sequence  | HMM, structured perceptron, (window-based classifier) | **structured prediction** (millions of choices) |\n",
    "|   NER  | CRF, structured perceptron | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "Remember: also think about the evaluation measure! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 2 and 5: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Simon Paarlberg's [blog on LSA](https://simonpaarlberg.com/post/latent-semantic-analyses/)\n",
    "* Richard Socher's [lecture 2](https://www.youtube.com/watch?v=xhHOL3TNyJs)\n",
    "* Graham Neubig's slides on the [structured perceptron](http://www.phontron.com/slides/nlp-programming-en-12-struct.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
