{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important concepts of lecture 4:\n",
    "\n",
    "* from n-hot vectors (sparse) to embeddings (dense inputs)\n",
    "* baselines for your projects (classification, sequence prediction)\n",
    "* short introduction to RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, what is the input $\\textbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Features so far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we go further, lets make a detour and recap: How did we represent a training instance in a traditional classifier so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, recall our example from week 1: training a Logistic Regression classifier for sentiment classification. \n",
    "\n",
    "* Describe in words: what were the features we used? I.e., how did we represent a training instance $\\textbf{x}$?\n",
    "* How can you now describe the entire sentiment training data set as a matrix $X$, i.e.,  what are the rows and columns of $X$? $$ X = \\{\\mathbf{x_1}, ... , \\mathbf{x_n}\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far we used **sparse** inputs (n-hot encodings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know about **distributional similarity** (embeddings: --traditional:LSA--, --neural:word2vec--)\n",
    "* understand the difference between **discrete** (one-hot) and **dense** feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**discrete representation**\n",
    "\n",
    "$$\\mathbf{x}_{cat} = [0,0,0,0,0,0,1] $$\n",
    "$$\\mathbf{x}_{dog} = [0,0,0,0,1,0,0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**similarity** on discrete representations? $$\\mathbf{x}_{cat} \\wedge \\mathbf{x}_{dog} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Probably the biggest jump when moving from traditional linear models with sparse inputs to deep neural networks is to stop representing each feature as a unique dimension, but instead represent them as **dense vectors** (Goldberg, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of using discrete representations, we will **embed** words into a high-dimensional feature space and represent each word by a lower-dimensional dense *vector* (aka. *embedding*):\n",
    "<img src=\"http://ben.bolte.cc/resources/attention_rnn/word_vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>**\"You shall know a word by the company it keeps\"** (Firth, J. R. 1957:11)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/flÃ¸debolle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"The company it keeps\": word co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent the \"company\" of a word in terms of a word co-occurence matrix. On the rows we have the words, on the columns their context.\n",
    "\n",
    "**Contexts** can be of different types, for example:\n",
    "* entire documents\n",
    "* paragraphs\n",
    "* a window around the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"She likes Groningen .\", \"She likes Cockatoos .\", \"I enjoy flying .\", \"I enjoy good food\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good', '.', 'Groningen', 'I', 'enjoy', 'flying', 'likes', 'She', 'food', 'Cockatoos'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = set(np.concatenate([s.split() for s in corpus],0))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.]\n",
      " [ 1.  1.  0.  0.]\n",
      " [ 0.  0.  1.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "{'.': 0, 'Cockatoos': 1, 'Groningen': 2, 'I': 3, 'She': 4, 'enjoy': 5, 'flying': 6, 'food': 7, 'good': 8, 'likes': 9}\n"
     ]
    }
   ],
   "source": [
    "# lets build a co-occurence matrix \n",
    "# rows: indices of words\n",
    "# columns: each column is a document, register whether the word appeared in the context\n",
    "## (in practice: many more context, different weighting schemes etc..)\n",
    "w2i = {w: i for i,w in enumerate(sorted(vocab))}\n",
    "coocurrence_matrix = np.zeros((len(vocab),len(corpus)))\n",
    "for col_idx, sentence in enumerate(corpus):\n",
    "    sentence = sentence.split()\n",
    "    for word in sentence:\n",
    "        word_idx = w2i[word]\n",
    "        coocurrence_matrix[(word_idx,col_idx)] +=1\n",
    "print(coocurrence_matrix)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Co-occurence matrix\n",
    "\n",
    "* **dimensionality**: number of words $|V|$ (size of vocabulary) times number of documents (typically number of documents is huge)\n",
    "* we want to **reduce** its dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSA - Latent Semantic Analysis (Singular Value Decomposition - SVD)\n",
    "\n",
    "Approximate a matrix $\\mathbf{C}$ through a decomposition into three submatrices (**of smaller dimensionality**):\n",
    "\n",
    "$$\\mathbf{C} \\approx \\mathbf{U \\sum V^T}$$\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\">\n",
    "\n",
    "NB. $=$ should be $\\approx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# reduce space to, say, 2 dimensions (for simplicity here)\n",
    "U, Sigma, VT = randomized_svd(coocurrence_matrix, \n",
    "                              n_components=2, random_state=113)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualizing the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntJJ0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bb85278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of U: (10, 2)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'like'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1b5b16e340ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size of U:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vector for 'like':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"like\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vector for 'enjoy':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enjoy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'like'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words = list(vocab)\n",
    "for i, lab in enumerate(vocab):\n",
    "    plt.text(U[i,0],U[i,1], words[i])\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()\n",
    "print(\"size of U:\", U.shape)\n",
    "print(\"vector for 'like':\", U[w2i[\"like\"]])\n",
    "print(\"vector for 'enjoy':\", U[w2i[\"enjoy\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity\n",
    "\n",
    "**cosine** similarity \n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/eq1.png\">\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/vector_example2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Calculate the cosine distance between the words *good* and *enjoy* as well as *good* and *flying*. (Hint: you can use the *cosine* **distance** function from *scipy.spatial.distance*, notice it is 1 minus cosine similarity). What is the distance between a word and itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distances:\n",
      "good<>enjoy:     0.01\n",
      "good<>flying: 0.06\n",
      "good<>good: -0.00\n"
     ]
    }
   ],
   "source": [
    "## solution:\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "v_flying = U[w2i[\"flying\"]]\n",
    "v_enjoy = U[w2i[\"enjoy\"]]\n",
    "v_good = U[w2i[\"good\"]]\n",
    "\n",
    "print(\"cosine distances:\")\n",
    "print(\"good<>enjoy:     {0:.2f}\".format(cosine(v_good, v_enjoy)))\n",
    "print(\"good<>flying: {0:.2f}\".format(cosine(v_good, v_flying)))\n",
    "print(\"good<>good: {0:.2f}\".format(cosine(v_good, v_good)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning approach: Directly learning word vectors (embeddings)\n",
    "\n",
    "* SVD: computation cost scales quadratically with size of co-occurence matrix; difficult to integrate new words\n",
    "* **Idea**: directly learn word vectors (word2vec)\n",
    "    * NLP (almost) from Scratch (Collobert & Weston, 2008)\n",
    "    * word2vec (Mikolov et al, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main idea of word2vec\n",
    "\n",
    "* instead of capturing co-occurence statistics of words\n",
    "* **predict context** (surrounding words of every word); in particular, predict words in a window of length $m$ around current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$o$ is the outside word (context), $c$ is the current center word; \n",
    "\n",
    "Maximize the probability of a word in the context ($o$) given the current word $c$:\n",
    "\n",
    "$$p(o|c) = \\frac{exp(u_o^T v_c)}{\\sum_{w=1}^W exp(u_w^T v_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://www.gabormelli.com/RKB/images/a/a6/skip-gram_NNLM_architecture.150216.jpg\" width=500>\n",
    "\n",
    "At the end you can read off the embedding vector from the Embedding layer! voila!\n",
    "\n",
    "NB. denominator $\\sum$ over all words! In practice, *negative sampling* is used (randomly choose a word which is not in context as a negative sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In deep learning we represent words as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**a) sparse representation vs b) dense representation**  (Figure 1 in Yoav Goldberg's primer)\n",
    "<img src=\"pics/sparsevsdense.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Traditional vs deep learning approach to feature extraction (representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The common pipeline of extracting features **for an NLP model with a Neural Network** then becomes:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a **vector** for **each feature** (lookup Embedding table)\n",
    "* **combine** vectors of features to get the vector representation for the **instance** $\\mathbf{x}$ (**dense representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets compare this to our traditional approach - the common pipeline of extracting features for an NLP model is:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector whose length is the total number of features with a 1 at position k if the k-th feature is active; this feature vector represents the **instance** $\\mathbf{x}$  (**sparse representation**, n-hot encoding)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "Now it should be clear why it is called sparse vs dense feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you combine different feature vector representations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In an NLP application, $\\mathbf{x}$ is usually composed of various embedding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the notation in Goldberg (2015), chapter 4, lets use the function $c(\\cdot)$ as **feature combiner** that creates our input embeddings layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common choice for $c$ is **concatenation**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f_3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "This is what happens if we use **Flatten** in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, $c$ could be the **sum of the embeddings vector**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1)+v(f_2)+v(f_3)] $\n",
    "\n",
    "or the **mean**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [mean(v(f_1),v(f_2),v(f_3))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In many papers $v$ is often referred to as the embeddings layer or lookup layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our example from before with explicit input representation\n",
    "\n",
    "For instance, let us explicitly state the input representation. Suppose we use the concatentation operator, then our network above becomes:\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "since: \n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "then: \n",
    "\n",
    "$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{[v(f_1); v(f_2); v(f_3)]W^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As computational graph:\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values of the *embedding vectors* (values of the vectors in Fig 1 b)) are treated as model parameters and trained together with the other parameters of the model (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unrolled (graph with concrete input, expected output, and loss node, Goldberg Figure 3 c):\n",
    "<img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: animacy classification\n",
    "\n",
    "Exercise: \n",
    "\n",
    "* add an embedding layer to the animacy classification example. For now use a simple concatenation as representation (Flatten the embedding layer). What performance do you get?\n",
    "\n",
    "* add code that reads off the embedding layer from the network and stores in in a file \"vectors.txt\". Once you have this embedding vector you can inspect it (find nearest neighbors) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Florida', 0.9196957945823669),\n",
       " ('Arlington', 0.9070820808410645),\n",
       " ('California', 0.8937997817993164),\n",
       " ('Buffalo', 0.8546850085258484),\n",
       " ('Canada', 0.8518165349960327),\n",
       " ('Japan', 0.8483419418334961),\n",
       " ('Europe', 0.8347345590591431),\n",
       " ('Houston', 0.8322141170501709),\n",
       " ('Virginia', 0.8314700126647949),\n",
       " ('France', 0.8283012509346008)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once we have read off the embeddings after training the animacy classifier, and \n",
    "# stored them in file 'vectors.txt' we load it for inspection\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['Texas'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elect', 0.851630449295044),\n",
       " ('word', 0.8355662822723389),\n",
       " ('letting', 0.8245967030525208),\n",
       " ('teach', 0.8199971914291382),\n",
       " ('swaying', 0.81275475025177),\n",
       " ('parks', 0.811647891998291),\n",
       " ('finally', 0.7990972995758057),\n",
       " ('wishes', 0.793840765953064),\n",
       " ('repainted', 0.7927937507629395),\n",
       " ('jus-', 0.787428617477417)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['send'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These vectors are not traditional word vectors learned with word2vec (skipgrams), instead we read them off from our animacy classifier (they are not trained with the word2vec objective, but are a by-product from the classifier). Nevertheless this shows us that we can also get embeddings from a neural network with dense (embedding) inputs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, in deep learning approaches to NLP words are represented as dense vectors. Where do these word vectors (embeddings) come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **randomly initialized** (small numbers around 0) and *trained with the network*\n",
    "* **off-the-shelf embeddings**: you can also use already trained, available embeddings (e.g. estimated with *word2vec*) and *initialize* the embedding layer of the network with your pretrained (unsupervised) word embeddings\n",
    "* **task-specific embeddings**: you could also train your embeddings, read them off the network, and use them for another task (or in a multi-task setup, more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inputs of different lengths\n",
    "\n",
    "In our animacy classification example we have one simplification: the input is always of the same size (namely, 5 words). \n",
    "\n",
    "However, in NLP we typically never have fixed size inputs, sentences are of different length. The neural network however needs inputs of fixed size. So how to deal with it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* create an input of fixed size, like using the mean embedding vector\n",
    "* use a model that can deal with variable size inputs, like a recurrent neural network (depending on the deep learning toolkit you use, you might still need to *pad* sequences to a fixed length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Karpathy's illustration of RNNs:\n",
    "<img src=\"http://benjaminbolte.com/resources/attention_rnn/karpathy_rnn.jpeg\">\n",
    "\n",
    "* From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important concepts: Prediction problems, non-neural baselines\n",
    "\n",
    "In NLP we typically deal with the following **prediction problems** - Given $x$, predict $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "| Given $x$ | predict $y$  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   a book review  | positive, negative | **classification** (binary) |\n",
    "|   a tweet  | language | **multi-class classification** (several choices) |\n",
    "|   a sentence  | its syntactic parse tree | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sequence tagging is also a structured prediction problem.\n",
    "\n",
    "For a sequence of n words with just 2 possible tags, how many possible sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "| Example task | Traditional classifier  | Type of prediction problem | \n",
    "|------|------|\n",
    "|   sentiment | Logistic regression, SVM | **classification** (binary) |\n",
    "|   language identification  | Logistic regression, SVM  | **multi-class classification** (several choices) |\n",
    "|   POS sequence  | HMM, structured perceptron, (window-based classifier) | **structured prediction** (millions of choices) |\n",
    "|   NER  | CRF, structured perceptron | **structured prediction** (millions of choices) |\n",
    "\n",
    "\n",
    "Remember: also think about the evaluation measure! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 2 and 5: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Simon Paarlberg's [blog on LSA](https://simonpaarlberg.com/post/latent-semantic-analyses/)\n",
    "* Richard Socher's [lecture 2](https://www.youtube.com/watch?v=xhHOL3TNyJs)\n",
    "* Graham Neubig's slides on the [structured perceptron](http://www.phontron.com/slides/nlp-programming-en-12-struct.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
