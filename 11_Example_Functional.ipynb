{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment example \n",
    "\n",
    "First lets load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "positive_sentences = [l.strip() for l in open(\"exercise/rt-polaritydata/rt-polarity.pos\").readlines()]\n",
    "negative_sentences = [l.strip() for l in open(\"exercise/rt-polaritydata/rt-polarity.neg\").readlines()]\n",
    "\n",
    "positive_labels = [1 for sentence in positive_sentences]\n",
    "negative_labels = [0 for sentence in negative_sentences]\n",
    "\n",
    "sentences = np.concatenate([positive_sentences,negative_sentences], axis=0)\n",
    "labels = np.concatenate([positive_labels,negative_labels],axis=0)\n",
    "\n",
    "## make sure we have a label for every data instance\n",
    "assert(len(sentences)==len(labels))\n",
    "data={}\n",
    "np.random.seed(113) #seed\n",
    "data['target']= np.random.permutation(labels)\n",
    "np.random.seed(113) # use same seed!\n",
    "data['data'] = np.random.permutation(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_rest, X_test, y_rest, y_test = train_test_split(data['data'], data['target'], test_size=0.2)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_rest, y_rest, test_size=0.2)\n",
    "del X_rest, y_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"#train instances: {} #dev: {} #test: {}\".format(len(X_train),len(X_dev),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word unigram (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "\n",
    "PAD = w2i[\"<pad>\"] # index 0 is padding\n",
    "UNK = w2i[\"<unk>\"] # index 1 is for UNK\n",
    "\n",
    "# convert words to indices, taking care of UNKs\n",
    "X_train_num = [[w2i[word] for word in sentence.split(\" \")] for sentence in X_train]\n",
    "w2i = defaultdict(lambda: UNK, w2i) # freeze - cute trick!\n",
    "X_dev_num = [[w2i[word] for word in sentence.split(\" \")] for sentence in X_dev]\n",
    "X_test_num = [[w2i[word] for word in sentence.split(\" \")] for sentence in X_test]\n",
    "\n",
    "max_sentence_length=max([len(s.split(\" \")) for s in X_train] \n",
    "                        + [len(s.split(\" \")) for s in X_dev] \n",
    "                        + [len(s.split(\" \")) for s in X_test] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# pad X\n",
    "X_train_pad = sequence.pad_sequences(X_train_num, maxlen=max_sentence_length, value=PAD)\n",
    "X_dev_pad = sequence.pad_sequences(X_dev_num, maxlen=max_sentence_length, value=PAD)\n",
    "X_test_pad = sequence.pad_sequences(X_test_num, maxlen=max_sentence_length,value=PAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(w2i)\n",
    "embeds_size=64\n",
    "\n",
    "np.random.seed(113) #set seed before any keras import\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, GlobalAveragePooling1D, LSTM\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embeds_size, input_length=max_sentence_length, mask_zero=True))\n",
    "#model.add(GlobalAveragePooling1D()) # mean embedding: actually better for this example!\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_pad, y_train, epochs=5)\n",
    "loss, accuracy = model.evaluate(X_dev_pad, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_dev_pad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([x[0] for x in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple inputs (characters and words) with an LSTM\n",
    "\n",
    "First lets prepare the data for the character representation (here we represent each words by the character indices). For this we use a dedicated c2i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# convert words to indices, taking care of UNKs\n",
    "def get_characters(sentence, c2i):\n",
    "    out = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        chars = []\n",
    "        for c in word:\n",
    "            chars.append(c2i[c])\n",
    "        out.append(chars)\n",
    "    return out\n",
    "\n",
    "c2i = defaultdict(lambda: len(c2i))\n",
    "\n",
    "PAD = c2i[\"<pad>\"] # index 0 is padding\n",
    "UNK = c2i[\"<unk>\"] # index 1 is for UNK\n",
    "X_train_num = [get_characters(sentence, c2i) for sentence in X_train]\n",
    "c2i = defaultdict(lambda: UNK, c2i) # freeze - cute trick!\n",
    "X_dev_num = [get_characters(sentence, c2i) for sentence in X_dev]\n",
    "X_test_num = [get_characters(sentence, c2i) for sentence in X_test]\n",
    "\n",
    "max_sentence_length=max([len(s.split(\" \")) for s in X_train] \n",
    "                        + [len(s.split(\" \")) for s in X_dev] \n",
    "                        + [len(s.split(\" \")) for s in X_test] )\n",
    "max_word_length = max([len(word)  for sentence in X_train_num for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### we need both max sent and word length\n",
    "print(max_sentence_length)\n",
    "print(max_word_length)\n",
    "print(X_train[0:2])\n",
    "print(X_train_num[0:2]) # example how the first two sentences are encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_words(tensor_words, max_word_len, pad_symbol_id, max_sent_len=None):\n",
    "    \"\"\"\n",
    "    pad character list all to same word length\n",
    "    \"\"\"\n",
    "    padded = []\n",
    "    for words in tensor_words:\n",
    "        if max_sent_len: #pad all to same sentence length (insert empty word list)\n",
    "            words = [[[0]]*(max_sent_len-len(words))+ words][0] #prepending empty words\n",
    "        padded.append(sequence.pad_sequences(words, maxlen=max_word_len, value=pad_symbol_id))\n",
    "    return np.array(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_pad_char = pad_words(X_train_num, max_word_length, 0, max_sent_len=max_sentence_length)\n",
    "X_dev_pad_char = pad_words(X_dev_num, max_word_length, 0, max_sent_len=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_pad_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## lets look at the first instance\n",
    "X_train_pad_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, LSTM, TimeDistributed, Flatten\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "max_chars = len(c2i)\n",
    "c_dim=32\n",
    "w_dim=64\n",
    "h_dim=100\n",
    "char_vocab_size = len(c2i)\n",
    "word_vocab_size = len(w2i)\n",
    "\n",
    "## lower-level character LSTM\n",
    "input_chars = Input(batch_shape=(batch_size, max_sentence_length, max_word_length), dtype='int32', name='input_chars')\n",
    "emb_chars = TimeDistributed(Embedding(input_dim=char_vocab_size, output_dim=c_dim, mask_zero=True), name='char_emb')(input_chars)\n",
    "flatten = TimeDistributed(Flatten(), name='flatten')(emb_chars)\n",
    "char_lstm = LSTM(h_dim, name='char_lstm')(flatten)\n",
    "#char_lstm = TimeDistributed(LSTM(h_dim))(emb_chars) # how to take 4-d input?\n",
    "\n",
    "#cmodel = Sequential()\n",
    "#cmodel.add(TimeDistributed(Embedding(input_dim=char_vocab_size, output_dim=c_dim, mask_zero=True), batch_input_shape=(batch_size, max_sentence_length, max_word_length), input_dtype='int32'))\n",
    "#cmodel.add(TimeDistributed(Flatten()))\n",
    "#cmodel.add(LSTM(h_dim))\n",
    "##cmodel.add(TimeDistributed(LSTM(h_dim)))\n",
    "##cmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "## input for words\n",
    "input_words = Input(batch_shape=(batch_size, max_sentence_length), name='input_words')\n",
    "emb_words = Embedding(input_dim=word_vocab_size, output_dim=c_dim, mask_zero=True, input_length=max_sentence_length, name='word_emb')(input_words)\n",
    "word_lstm = LSTM(h_dim, name='word_lstm')(emb_words)\n",
    "#wmodel = Sequential()\n",
    "#wmodel.add(Embedding(input_dim=word_vocab_size, output_dim=c_dim, mask_zero=True, input_length=max_sentence_length))\n",
    "#wmodel.add(LSTM(h_dim))\n",
    "##wmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# how to do the same in a sequential model? (especially the concatenate merge?)\n",
    "#merge = Sequential()\n",
    "##merge.add(InputLayer(shape=(batch_size, max_sentence_length)))\n",
    "##merge = Concatenate([cmodel, wmodel])\n",
    "#merge.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# We can then concatenate the two vectors:\n",
    "merged_vector = keras.layers.concatenate([char_lstm, word_lstm], axis=-1)\n",
    "\n",
    "# And add a prediction node on top\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_chars, input_words], outputs=predictions)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit([X_train_pad_char, X_train_pad], y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate([X_dev_pad_char, X_dev_pad], y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todo: find optimal parameters + model structure on dev, evaluate final model on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing words only out of characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a separate word embedding matrix, compose words through characters (see https://aclweb.org/anthology/W/W16/W16-4303.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "max_chars = len(c2i)\n",
    "c_dim=50\n",
    "c_h_dim=256\n",
    "w_h_dim=256\n",
    "char_vocab_size = len(c2i)\n",
    "\n",
    "## lower-level character LSTM\n",
    "input_chars = Input(shape=(max_sentence_length, max_word_length))\n",
    "\n",
    "embedded_chars = TimeDistributed(Embedding(char_vocab_size, c_dim,\n",
    "                                         input_length=max_word_length))(input_chars)\n",
    "char_lstm = TimeDistributed(LSTM(c_h_dim))(embedded_chars)\n",
    "\n",
    "word_lstm_from_char = LSTM(w_h_dim)(char_lstm)\n",
    "\n",
    "# And add a prediction node on top\n",
    "predictions = Dense(1, activation='sigmoid')(word_lstm_from_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train_pad_char.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=input_chars, outputs=predictions)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_pad_char, y_train, epochs=5, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_dev_pad_char, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: doesn't work very well yet, check! plus find optimal parameters + model structure on dev, evaluate final model on test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
